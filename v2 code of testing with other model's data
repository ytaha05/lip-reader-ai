import cv2
import mediapipe as mp
import streamlit as st
import numpy as np
from collections import deque

st.title("SilentAid â€“ AI-Powered Lip Reading System")

# Initialize MediaPipe
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(
    static_image_mode=False,
    max_num_faces=1,
    refine_landmarks=True,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

# Lip landmark indices
LIPS_LANDMARKS = set()
for pair in mp_face_mesh.FACEMESH_LIPS:
    LIPS_LANDMARKS.update(pair)
LIPS_LANDMARKS = sorted(list(LIPS_LANDMARKS))

class AILipReader:
    def __init__(self):
        self.sequence_buffer = deque(maxlen=30)
        # Updated word patterns with refined ranges
        self.word_patterns = {
            "help": {
                "segment1": {"avg_movement": (5, 10), "avg_intensity": (80, 120), "intensity_change": (-10, 10), "rate_of_change": (-2, 2)},
                "segment2": {"avg_movement": (0, 5), "avg_intensity": (60, 100), "intensity_change": (-10, 10), "rate_of_change": (-1, 1)},
                "segment3": {"avg_movement": (8, 15), "avg_intensity": (80, 120), "intensity_change": (-100, -20), "rate_of_change": (-15, -5)}  # Sharp closing for "p"
            },
            "no": {
                "segment1": {"avg_movement": (4, 9), "avg_intensity": (80, 120), "intensity_change": (-5, 5), "rate_of_change": (-1, 1)},
                "segment2": {"avg_movement": (6, 12), "avg_intensity": (90, 130), "intensity_change": (-10, 10), "rate_of_change": (-3, 3)},  # Gradual motion
                "segment3": {"avg_movement": (2, 6), "avg_intensity": (70, 110), "intensity_change": (-10, 0), "rate_of_change": (-2, 0)}  # Smoother closing
            },
            "water": {
                "segment1": {"avg_movement": (5, 10), "avg_intensity": (80, 120), "intensity_change": (-10, 10), "rate_of_change": (-2, 2)},
                "segment2": {"avg_movement": (10, 20), "avg_intensity": (120, 160), "intensity_change": (10, 50), "rate_of_change": (2, 5)},
                "segment3": {"avg_movement": (2, 8), "avg_intensity": (60, 100), "intensity_change": (-50, -10), "rate_of_change": (-3, -1)}
            },
            "yes": {
                "segment1": {"avg_movement": (3, 8), "avg_intensity": (70, 110), "intensity_change": (-5, 5), "rate_of_change": (-1, 1)},
                "segment2": {"avg_movement": (5, 10), "avg_intensity": (90, 130), "intensity_change": (0, 10), "rate_of_change": (0, 2)},
                "segment3": {"avg_movement": (2, 6), "avg_intensity": (80, 120), "intensity_change": (-10, 0), "rate_of_change": (-2, 0)}
            }
        }

    def extract_lip_region(self, frame, lip_landmarks):
        if not lip_landmarks:
            return None
        lip_points = np.array(lip_landmarks)
        x, y, w, h = cv2.boundingRect(lip_points)
        padding = 20
        x = max(0, x - padding)
        y = max(0, y - padding)
        w = min(frame.shape[1] - x, w + 2*padding)
        h = min(frame.shape[0] - y, h + 2*padding)
        lip_region = frame[y:y+h, x:x+w]
        if lip_region.size > 0:
            lip_region = cv2.resize(lip_region, (64, 64))
            return lip_region
        return None
    
    def analyze_lip_movement(self, sequence):
        if len(sequence) < 10:
            return []
        movements = []
        intensities = []
        for i in range(1, len(sequence)):
            if sequence[i] is not None and sequence[i-1] is not None:
                diff = cv2.absdiff(sequence[i], sequence[i-1])
                movement = np.mean(diff)
                movements.append(movement)
                gray = cv2.cvtColor(sequence[i], cv2.COLOR_BGR2GRAY)
                intensity = np.mean(gray)
                intensities.append(intensity)
        if not movements or not intensities:
            return []
        num_frames = len(movements)
        segment_size = num_frames // 3
        segments = [
            (0, segment_size),
            (segment_size, 2 * segment_size),
            (2 * segment_size, num_frames)
        ]
        segment_features = []
        for start, end in segments:
            seg_movements = movements[start:end]
            seg_intensities = intensities[start:end]
            if seg_movements and seg_intensities:
                avg_movement = np.mean(seg_movements)
                avg_intensity = np.mean(seg_intensities)
                intensity_change = seg_intensities[-1] - seg_intensities[0] if len(seg_intensities) > 1 else 0
                rate_of_change = np.mean([seg_intensities[i] - seg_intensities[i-1] for i in range(1, len(seg_intensities))]) if len(seg_intensities) > 1 else 0
                segment_features.append({
                    "avg_movement": avg_movement,
                    "avg_intensity": avg_intensity,
                    "intensity_change": intensity_change,
                    "rate_of_change": rate_of_change
                })
            else:
                segment_features.append(None)
        predictions = self.pattern_matching(segment_features)
        return predictions
    
    def pattern_matching(self, segment_features):
        predictions = []
        for word, pattern in self.word_patterns.items():
            confidence = 0.0
            for i, seg in enumerate(["segment1", "segment2", "segment3"]):
                if segment_features[i] is not None:
                    seg_conf = 0.0
                    features = segment_features[i]
                    pattern_seg = pattern[seg]
                    move_min, move_max = pattern_seg["avg_movement"]
                    if move_min <= features["avg_movement"] <= move_max:
                        seg_conf += 0.2
                    int_min, int_max = pattern_seg["avg_intensity"]
                    if int_min <= features["avg_intensity"] <= int_max:
                        seg_conf += 0.2
                    change_min, change_max = pattern_seg["intensity_change"]
                    if change_min <= features["intensity_change"] <= change_max:
                        seg_conf += 0.2
                    roc_min, roc_max = pattern_seg["rate_of_change"]
                    if roc_min <= features["rate_of_change"] <= roc_max:
                        seg_conf += 0.4  # Increased weight for rate of change
                    confidence += seg_conf / 3
            if confidence > 0.1:
                predictions.append((word, min(confidence, 0.95)))
        predictions.sort(key=lambda x: x[1], reverse=True)
        return predictions[:5]
    
    def process_frame(self, frame, lip_landmarks):
        lip_region = self.extract_lip_region(frame, lip_landmarks)
        if lip_region is not None:
            self.sequence_buffer.append(lip_region)
        if len(self.sequence_buffer) >= 10:
            predictions = self.analyze_lip_movement(list(self.sequence_buffer))
            return predictions
        return []

# Streamlit Interface
st.sidebar.header("AI Lip Reading Controls")
mode = st.sidebar.selectbox("Select Mode", ["Real-time Recognition", "Advanced Settings"])

if mode == "Real-time Recognition":
    st.header("AI-Powered Real-time Recognition")
    if 'ai_lip_reader' not in st.session_state:
        st.session_state.ai_lip_reader = AILipReader()
    run = st.checkbox('Start AI Recognition')
    confidence_threshold = st.slider("Confidence threshold", 0.0, 1.0, 0.4)
    FRAME_WINDOW = st.image([])
    col1, col2 = st.columns(2)
    with col1:
        prediction_display = st.empty()
    with col2:
        confidence_display = st.empty()
    lip_region_display = st.empty()
    if run:
        cap = cv2.VideoCapture(0)
        while run:
            success, frame = cap.read()
            if not success:
                st.warning("Webcam not detected.")
                break
            frame = cv2.flip(frame, 1)
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = face_mesh.process(rgb)
            predictions = None
            if results.multi_face_landmarks:
                for face_landmarks in results.multi_face_landmarks:
                    h, w, _ = frame.shape
                    lip_points = []
                    for idx in LIPS_LANDMARKS:
                        x = int(face_landmarks.landmark[idx].x * w)
                        y = int(face_landmarks.landmark[idx].y * h)
                        lip_points.append((x, y))
                        cv2.circle(frame, (x, y), 1, (0, 255, 0), -1)
                    if lip_points:
                        lip_np = np.array(lip_points)
                        x, y, w_lip, h_lip = cv2.boundingRect(lip_np)
                        cv2.rectangle(frame, (x, y), (x + w_lip, y + h_lip), (0, 255, 0), 2)
                        predictions = st.session_state.ai_lip_reader.process_frame(frame, lip_points)
                        if predictions and len(predictions) > 0:
                            best_pred, best_conf = predictions[0]
                            if best_conf > confidence_threshold:
                                cv2.putText(frame, f"AI: {best_pred}", 
                                          (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                                cv2.putText(frame, f"Conf: {best_conf:.2f}", 
                                          (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                                prediction_display.success(f"**Detected: {best_pred}**")
                                pred_text = "\n".join([f"{word}: {conf:.2f}" for word, conf in predictions[:3]])
                                confidence_display.info(f"Top Predictions:\n{pred_text}")
                            else:
                                prediction_display.info("Analyzing...")
                                if predictions:
                                    confidence_display.info(f"Best: {predictions[0][0]} ({predictions[0][1]:.2f})")
            FRAME_WINDOW.image(frame, channels="BGR")
            buffer_size = len(st.session_state.ai_lip_reader.sequence_buffer)
            lip_region_display.info(f"Buffer: {buffer_size}/30 frames")
        cap.release()

elif mode == "Advanced Settings":
    st.header("Advanced AI Settings")
    st.subheader("Model Configuration")
    st.write("Current setup uses refined heuristics with emphasis on rate of change.")
    st.markdown("""
    - **Adjusted Patterns**: Distinct ranges for 'no' and 'help'.
    - **Weighted Features**: Higher emphasis on rate of change.
    """)

st.sidebar.markdown("""
## AI Lip Reading
**Tips:**
- Face the camera directly
- Speak "no" with a smooth, rounded motion
- Ensure good lighting
""")
