#fix data collection

# pip install streamlit opencv-python mediapipe numpy tensorflow scikit-learn

import cv2
import mediapipe as mp
import streamlit as st
import numpy as np
import os
import time
import pickle
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import joblib

st.title("SilentAid â€“ Complete Lip Reading System")

# Initialize MediaPipe
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False,
                                   max_num_faces=1,
                                   refine_landmarks=True,
                                   min_detection_confidence=0.5,
                                   min_tracking_confidence=0.5)

# Create directories
os.makedirs("lip_data", exist_ok=True)
os.makedirs("models", exist_ok=True)

# Get lip landmark indices
LIPS_LANDMARKS = set()
for pair in mp_face_mesh.FACEMESH_LIPS:
    LIPS_LANDMARKS.update(pair)

# Convert to sorted list for consistent ordering
LIPS_LANDMARKS = sorted(list(LIPS_LANDMARKS))

def extract_features(lip_sequence):
    """Extract features from lip landmark sequence"""
    if len(lip_sequence) == 0:
        return np.zeros(len(LIPS_LANDMARKS) * 4)  # x, y, dx, dy for each landmark
    
    features = []
    lip_array = np.array(lip_sequence)
    
    # Get mean position for each landmark
    mean_positions = np.mean(lip_array, axis=0)
    
    # Get movement features (differences between consecutive frames)
    if len(lip_sequence) > 1:
        movements = np.diff(lip_array, axis=0)
        mean_movement = np.mean(movements, axis=0)
        std_movement = np.std(movements, axis=0)
    else:
        mean_movement = np.zeros_like(mean_positions)
        std_movement = np.zeros_like(mean_positions)
    
    # Flatten all features
    features.extend(mean_positions.flatten())
    features.extend(mean_movement.flatten())
    features.extend(std_movement.flatten())
    
    # Add sequence length as a feature
    features.append(len(lip_sequence))
    
    return np.array(features)

def normalize_lip_points(lip_points):
    """Normalize lip points relative to center and scale"""
    if len(lip_points) == 0:
        return lip_points
    
    lip_array = np.array(lip_points)
    center = np.mean(lip_array, axis=0)
    
    # Center the points
    centered = lip_array - center
    
    # Scale by the maximum distance from center
    max_dist = np.max(np.sqrt(np.sum(centered**2, axis=1)))
    if max_dist > 0:
        normalized = centered / max_dist
    else:
        normalized = centered
    
    return normalized.tolist()

# Streamlit interface
st.sidebar.header("Controls")
mode = st.sidebar.selectbox("Select Mode", ["Data Collection", "Training", "Recognition"])

if mode == "Data Collection":
    st.header("Data Collection Mode")
    
    # Controls
    col1, col2 = st.columns(2)
    with col1:
        run = st.checkbox('Start Camera')
        label = st.text_input("Enter word you're saying:", value="")
        record = st.checkbox("Record Sequence")
    
    with col2:
        save_button = st.button("Save Sequence")
        clear_button = st.button("Clear Current Sequence")
        min_frames = st.slider("Minimum frames to record", 10, 100, 30)
    
    # Status and display
    FRAME_WINDOW = st.image([])
    status = st.empty()
    
    # Initialize session state
    if 'sequence' not in st.session_state:
        st.session_state.sequence = []
    
    cap = cv2.VideoCapture(0)
    
    while run:
        success, frame = cap.read()
        if not success:
            st.warning("Webcam not detected.")
            break
        
        frame = cv2.flip(frame, 1)
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = face_mesh.process(rgb)
        
        if results.multi_face_landmarks:
            for face_landmarks in results.multi_face_landmarks:
                h, w, _ = frame.shape
                lip_points = []
                
                for idx in LIPS_LANDMARKS:
                    x = int(face_landmarks.landmark[idx].x * w)
                    y = int(face_landmarks.landmark[idx].y * h)
                    lip_points.append((x, y))
                    cv2.circle(frame, (x, y), 1, (0, 255, 0), -1)
                
                if lip_points:
                    # Draw bounding box
                    lip_np = np.array(lip_points)
                    x, y, w_lip, h_lip = cv2.boundingRect(lip_np)
                    cv2.rectangle(frame, (x, y), (x + w_lip, y + h_lip), (0, 0, 255), 2)
                    
                    # Record if enabled
                    if record and label:
                        normalized_points = normalize_lip_points(lip_points)
                        st.session_state.sequence.append(normalized_points)
                        
                        # Show recording status
                        cv2.putText(frame, f"RECORDING: {len(st.session_state.sequence)} frames", 
                                  (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                        status.info(f"Recording '{label}': {len(st.session_state.sequence)} frames")
        
        FRAME_WINDOW.image(frame, channels="BGR")
    
    cap.release()
    
    # Handle buttons
    if clear_button:
        st.session_state.sequence = []
        status.info("Sequence cleared!")
    
    if save_button and len(st.session_state.sequence) >= min_frames and label:
        filename = f"lip_data/{label}_{int(time.time())}.pkl"
        with open(filename, 'wb') as f:
            pickle.dump({
                'label': label,
                'sequence': st.session_state.sequence,
                'timestamp': time.time()
            }, f)
        status.success(f"Saved {len(st.session_state.sequence)} frames as {filename}")
        st.session_state.sequence = []
    elif save_button and len(st.session_state.sequence) < min_frames:
        status.error(f"Need at least {min_frames} frames to save!")
    elif save_button and not label:
        status.error("Please enter a label!")

elif mode == "Training":
    st.header("Model Training")
    
    # Load all data files (both .pkl and .npy formats)
    pkl_files = [f for f in os.listdir("lip_data") if f.endswith('.pkl')]
    npy_files = [f for f in os.listdir("lip_data") if f.endswith('.npy')]
    
    total_files = len(pkl_files) + len(npy_files)
    
    if total_files == 0:
        st.warning("No training data found! Please collect some data first.")
    else:
        st.write(f"Found {len(pkl_files)} .pkl files and {len(npy_files)} .npy files")
        
        # Show data summary
        word_counts = {}
        
        # Count words from pkl files
        for file in pkl_files:
            word = file.split('_')[0]
            word_counts[word] = word_counts.get(word, 0) + 1
            
        # Count words from npy files
        for file in npy_files:
            word = file.split('_')[0]
            word_counts[word] = word_counts.get(word, 0) + 1
        
        st.write("Data distribution:", word_counts)
        
        if st.button("Train Model"):
            with st.spinner("Training model..."):
                # Load all data
                X = []
                y = []
                
                # Load pkl files
                for file in pkl_files:
                    try:
                        with open(f"lip_data/{file}", 'rb') as f:
                            data = pickle.load(f)
                            features = extract_features(data['sequence'])
                            X.append(features)
                            y.append(data['label'])
                    except Exception as e:
                        st.warning(f"Error loading {file}: {e}")
                
                # Load npy files
                for file in npy_files:
                    try:
                        # Load the npy data
                        sequence_data = np.load(f"lip_data/{file}")
                        
                        # Extract label from filename
                        label = file.split('_')[0]
                        
                        # Convert to list format and normalize if needed
                        if len(sequence_data.shape) == 3:  # Shape: (frames, landmarks, 2)
                            sequence = []
                            for frame in sequence_data:
                                normalized_points = normalize_lip_points(frame.tolist())
                                sequence.append(normalized_points)
                        else:
                            # If it's already in the right format
                            sequence = sequence_data.tolist()
                        
                        features = extract_features(sequence)
                        X.append(features)
                        y.append(label)
                    except Exception as e:
                        st.warning(f"Error loading {file}: {e}")
                
                if len(X) == 0:
                    st.error("No valid data could be loaded!")
                else:
                    X = np.array(X)
                    y = np.array(y)
                    
                    st.write(f"Loaded {len(X)} samples with {len(set(y))} different words")
                    
                    # Check if we have enough data for splitting
                    if len(set(y)) < 2:
                        st.error("Need at least 2 different words to train the model!")
                    else:
                        # Split data - handle cases where stratification isn't possible
                        unique_classes = len(set(y))
                        test_size = min(0.2, (len(X) - unique_classes) / len(X))
                        
                        if len(X) < 10 or test_size <= 0:
                            # If very little data, use all for training
                            X_train, X_test = X, X
                            y_train, y_test = y, y
                            st.warning("Very little data - using all data for both training and testing")
                        else:
                            # Check if we can do stratified split
                            min_class_count = min([list(y).count(cls) for cls in set(y)])
                            
                            if min_class_count >= 2 and len(X) * test_size >= unique_classes:
                                # Can do stratified split
                                X_train, X_test, y_train, y_test = train_test_split(
                                    X, y, test_size=test_size, random_state=42, stratify=y
                                )
                            else:
                                # Do regular split without stratification
                                X_train, X_test, y_train, y_test = train_test_split(
                                    X, y, test_size=test_size, random_state=42
                                )
                                st.info("Using random split (not stratified) due to class distribution")
                        
                        # Scale features
                        scaler = StandardScaler()
                        X_train_scaled = scaler.fit_transform(X_train)
                        X_test_scaled = scaler.transform(X_test)
                        
                        # Train model
                        model = RandomForestClassifier(n_estimators=100, random_state=42)
                        model.fit(X_train_scaled, y_train)
                        
                        # Evaluate
                        train_pred = model.predict(X_train_scaled)
                        test_pred = model.predict(X_test_scaled)
                        
                        train_acc = accuracy_score(y_train, train_pred)
                        test_acc = accuracy_score(y_test, test_pred)
                        
                        # Save model and scaler
                        joblib.dump(model, 'models/lip_reading_model.pkl')
                        joblib.dump(scaler, 'models/scaler.pkl')
                        joblib.dump(list(set(y)), 'models/classes.pkl')
                        
                        st.success(f"Model trained successfully!")
                        st.write(f"Training accuracy: {train_acc:.2f}")
                        st.write(f"Testing accuracy: {test_acc:.2f}")
                        st.write(f"Classes: {list(set(y))}")
                        
                        # Show feature importance
                        feature_importance = model.feature_importances_
                        st.write("Model trained and saved!")

elif mode == "Recognition":
    st.header("Real-time Recognition")
    
    # Check if model exists
    if not os.path.exists('models/lip_reading_model.pkl'):
        st.error("No trained model found! Please train a model first.")
    else:
        # Load model
        model = joblib.load('models/lip_reading_model.pkl')
        scaler = joblib.load('models/scaler.pkl')
        classes = joblib.load('models/classes.pkl')
        
        st.write(f"Model loaded. Can recognize: {classes}")
        
        # Controls
        run = st.checkbox('Start Recognition')
        recognition_threshold = st.slider("Confidence threshold", 0.0, 1.0, 0.3)
        sequence_length = st.slider("Sequence length for prediction", 10, 50, 20)
        
        FRAME_WINDOW = st.image([])
        prediction_display = st.empty()
        confidence_display = st.empty()
        
        # Initialize sequence buffer
        if 'recognition_sequence' not in st.session_state:
            st.session_state.recognition_sequence = []
        
        cap = cv2.VideoCapture(0)
        
        while run:
            success, frame = cap.read()
            if not success:
                st.warning("Webcam not detected.")
                break
            
            frame = cv2.flip(frame, 1)
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = face_mesh.process(rgb)
            
            if results.multi_face_landmarks:
                for face_landmarks in results.multi_face_landmarks:
                    h, w, _ = frame.shape
                    lip_points = []
                    
                    for idx in LIPS_LANDMARKS:
                        x = int(face_landmarks.landmark[idx].x * w)
                        y = int(face_landmarks.landmark[idx].y * h)
                        lip_points.append((x, y))
                        cv2.circle(frame, (x, y), 1, (0, 255, 0), -1)
                    
                    if lip_points:
                        # Draw bounding box
                        lip_np = np.array(lip_points)
                        x, y, w_lip, h_lip = cv2.boundingRect(lip_np)
                        cv2.rectangle(frame, (x, y), (x + w_lip, y + h_lip), (0, 255, 0), 2)
                        
                        # Add to sequence buffer
                        normalized_points = normalize_lip_points(lip_points)
                        st.session_state.recognition_sequence.append(normalized_points)
                        
                        # Keep only recent frames
                        if len(st.session_state.recognition_sequence) > sequence_length:
                            st.session_state.recognition_sequence.pop(0)
                        
                        # Make prediction if we have enough frames
                        if len(st.session_state.recognition_sequence) >= sequence_length:
                            features = extract_features(st.session_state.recognition_sequence)
                            features_scaled = scaler.transform([features])
                            
                            prediction_proba = model.predict_proba(features_scaled)[0]
                            max_confidence = np.max(prediction_proba)
                            predicted_class = classes[np.argmax(prediction_proba)]
                            
                            # Display prediction if confidence is high enough
                            if max_confidence > recognition_threshold:
                                cv2.putText(frame, f"Prediction: {predicted_class}", 
                                          (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                                cv2.putText(frame, f"Confidence: {max_confidence:.2f}", 
                                          (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                                
                                prediction_display.success(f"**Detected: {predicted_class}**")
                                confidence_display.info(f"Confidence: {max_confidence:.2f}")
                            else:
                                prediction_display.info("Listening...")
                                confidence_display.info(f"Max confidence: {max_confidence:.2f}")
            
            FRAME_WINDOW.image(frame, channels="BGR")
        
        cap.release()

# Instructions
st.sidebar.markdown("""
## Instructions:

**Data Collection:**
1. Enter a word you want to teach
2. Start camera and click "Record Sequence"
3. Mouth the word clearly for 2-3 seconds
4. Click "Save Sequence"
5. Repeat for multiple words and multiple samples

**Training:**
1. Collect data for at least 2 different words
2. Click "Train Model" to create your lip reading model

**Recognition:**
1. Start recognition mode
2. The system will predict what you're saying in real-time
3. Adjust confidence threshold as needed
""")
